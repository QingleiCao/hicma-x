extern "C" %{
/**
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2023-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2023-2025     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 **/

#include "climate_emulator.h"

/**
 * @brief Copy data from flm matrix to flmT matrix with proper offset handling
 * 
 * This function copies a block of data from the flm matrix to the flmT matrix,
 * handling the case where the last tile might have different dimensions.
 * 
 * @param flmT Destination matrix (flmT)
 * @param flm Source matrix (flm)
 * @param gb Climate emulator structure containing matrix descriptors
 * @param m Row index in the source matrix
 * @param n Column index in the destination matrix
 */
static void climate_emulator_flm2flmT(double *flmT, double *flm, climate_emulator_struct_t *gb, int m, int n) {
    // Calculate offsets for source and destination matrices
    int flm_offset = m * gb->desc_flmT.super.mb; 
    int flmT_offset = n * gb->desc_flmT.super.mb; 
    
    // Determine the size to copy, handling the last tile specially
    int size = (m == gb->desc_flmT.super.lmt-1)? (gb->flm_M*gb->flm_N)-flm_offset : gb->desc_flmT.super.mb; 
    
    // Copy the data block
    memcpy(flmT+flmT_offset, flm+flm_offset, size*sizeof(double));
}

%}

/* Global variable declarations for PaRSEC task scheduling
 * These define the data dependencies and matrix descriptors used by the tasks
 */
desc_f_data        [ type = "parsec_tiled_matrix_t*" ]
desc_flm           [ type = "parsec_tiled_matrix_t*" aligned = desc_f_data]
desc_flmT          [ type = "parsec_tiled_matrix_t*" ]

desc_Et1           [ type = "parsec_tiled_matrix_t*" ]
desc_Et2           [ type = "parsec_tiled_matrix_t*" ]
desc_Ep            [ type = "parsec_tiled_matrix_t*" ]
desc_Slmn          [ type = "parsec_tiled_matrix_t*" ]
desc_Ie            [ type = "parsec_tiled_matrix_t*" ]
desc_Io            [ type = "parsec_tiled_matrix_t*" ]
desc_P             [ type = "parsec_tiled_matrix_t*" ]
desc_D             [ type = "parsec_tiled_matrix_t*" ]

gb                 [ type = "climate_emulator_struct_t *" ]

/* Memory pools for temporary workspace buffers
 * These are used to allocate and manage temporary memory for computations
 */
p_Gmtheta_r        [ type = "parsec_memory_pool_t *" hidden = on default = NULL ]
p_Fmnm             [ type = "parsec_memory_pool_t *" hidden = on default = NULL ]
p_work_1           [ type = "parsec_memory_pool_t *" hidden = on default = NULL ]
p_work_2           [ type = "parsec_memory_pool_t *" hidden = on default = NULL ]

/* GPU workspace for CUDA/HIP acceleration */
ws_gpu       [ type = "void *" hidden = on default = NULL ]

/* GPU device management parameters */
nb_cuda_devices      [ type = "int"   hidden = on default = 0 ]
cuda_device_index    [ type = "int *" hidden = on default = "NULL"]

/* Limit the number of tasks for batch processing */
batch         [ type = "int" hidden = on default = 1 ]

/* GPU binding task for data placement optimization
 * This task ensures that data is placed on the appropriate GPU device
 * for better performance and load balancing across batches
 */
bind_gpu(n, k)

k = 0 .. batch-1

/* Calculate number of columns per batch */
num_col = %{ return desc_f_data->lnt/batch+1; %}
/* Calculate column range for current batch */
n = %{ return k*num_col; %} .. %{ return parsec_imin((k+1)*num_col-1, desc_f_data->lnt-1); %}

: desc_f_data(0, n)

READ flm   <- desc_flm(0, n)
           -> flm task(n, k)         [ type_remote = flm ]

READ f_data <- desc_f_data(0, n)
            -> f_data task(n, k)     [ type_remote = f_data ]

/* Control dependencies for batch processing:
 * Each batch waits for the previous batch to complete before proceeding
 * This ensures proper ordering of data processing across batches
 */
CTL ctl1 <- (k > 0)? ctl2 bind_gpu(n-num_col, k-1)
CTL ctl2 -> (k < batch-1)? ctl1 bind_gpu(n+num_col, k+1)

BODY
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)  || defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
    // GPU acceleration is available
    if( nb_cuda_devices > 0 ) {
        // Determine which GPU to use for this task based on load balancing
        int g = climate_emulator_gpu_load_balance( n, gb->nodes, nb_cuda_devices );
        
        // Advise the runtime to place flm data on the selected GPU
        parsec_advise_data_on_device( _f_flm->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
        
        // Advise the runtime to place f_data on the selected GPU
        parsec_advise_data_on_device( _f_f_data->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
    }
#endif
}
END

/* Main computation task for forward geqsht operation
 * Each task processes one column (n) within a specific batch (k)
 * and applies the forward transformation using pre-computed matrices
 */
task(n, k)

k = 0 .. batch-1

/* Calculate number of columns per batch */
num_col = %{ return desc_f_data->lnt/batch+1; %}
/* Calculate column range for current batch */
n = %{ return k*num_col; %} .. %{ return parsec_imin((k+1)*num_col-1, desc_f_data->lnt-1); %}

/* Calculate the rank that owns the current column for proper data distribution */
my_rank = %{ return desc_f_data->super.rank_of(&desc_f_data->super, 0, n); %}

: desc_f_data(0, n)

READ f_data <- f_data bind_gpu(n, k)                    [ type_remote = f_data ]
RW flm    <- flm bind_gpu(n, k)                         [ type_remote = flm ]
          -> flm task_flmT(0..desc_flmT->lmt-1, n, k)   [ type_remote = flm ]
          -> desc_flm(0, n)

/* Read pre-computed matrices from the appropriate rank */
READ    Et1 <- desc_Et1(0, my_rank) 
READ    Et2 <- desc_Et2(0, my_rank)
READ    Ep  <- desc_Ep(0, my_rank) 
READ    Slmn <- desc_Slmn(0, my_rank) 
READ    Ie <- desc_Ie(0, my_rank) 
READ    Io <- desc_Io(0, my_rank) 
READ    P  <- desc_P(0, my_rank) 
READ    D  <- desc_D(0, my_rank) 

/* GPU implementation using CUDA */
BODY[type=CUDA]
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    climate_emulator_geqsht_forward_pre_computed_version_gpu_core(flm, f_data, Et1, Et2, Ep, Slmn, Ie, Io, P, D, cuda_device, gpu_task, cuda_stream, gb);
#endif
}
END

/* GPU implementation using HIP */
BODY[type=HIP]
{
#if defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
    climate_emulator_geqsht_forward_pre_computed_version_gpu_core(flm, f_data, Et1, Et2, Ep, Slmn, Ie, Io, P, D, cuda_device, gpu_task, cuda_stream, gb);
#endif
}
END

/* CPU implementation of the forward transformation */
BODY
{
    // Allocate temporary workspace from memory pools
    complex double *Gmtheta_r = (complex double *)parsec_private_memory_pop(p_Gmtheta_r);
    complex double *Fmnm = (complex double *)parsec_private_memory_pop(p_Fmnm);
    complex double *tmp1 = (complex double *)parsec_private_memory_pop(p_work_1);
    complex double *tmp2 = (complex double *)parsec_private_memory_pop(p_work_2);

    // Perform the forward geqsht transformation using pre-computed matrices
    climate_emulator_geqsht_forward_pre_computed_version_core(flm, f_data, Et1, Et2, Ep, Slmn, Ie, Io, P, D, Gmtheta_r, Fmnm, tmp1, tmp2, gb);

    // Return workspace to memory pools
    parsec_private_memory_push(p_Gmtheta_r, Gmtheta_r);
    parsec_private_memory_push(p_Fmnm, Fmnm);
    parsec_private_memory_push(p_work_1, tmp1);
    parsec_private_memory_push(p_work_2, tmp2);
}
END

/* Data transfer task for copying flm to flmT matrix
 * This task handles the data movement between different matrix representations
 * Each task processes one row (m) within a specific batch (k)
 */
task_flmT(m, n, k)

m = 0 .. desc_flmT->lmt-1
k = 0 .. batch-1

/* Calculate number of columns per batch */
num_col = %{ return desc_f_data->lnt/batch+1; %}
/* Calculate column range for current batch */
n = %{ return k*num_col; %} .. %{ return parsec_imin((k+1)*num_col-1, desc_f_data->lnt-1); %}

: desc_flmT(m, 0)

READ flm <- flm task(n, k)           [ type_remote = flm ]

READ flmT  <- desc_flmT(m, 0)
    //     -> desc_flmT(m, 0)   

BODY
{
    // Copy data from flm to flmT matrix
    climate_emulator_flm2flmT(flmT, flm, gb, m, n);
}
END

extern "C" %{

/**
 * @brief Create and initialize a new PaRSEC taskpool for forward geqsht operation
 * 
 * This function sets up the taskpool with all necessary matrix descriptors,
 * memory pools, and GPU workspace for the forward geqsht operation using
 * pre-computed matrices.
 * 
 * @param parsec PaRSEC context
 * @param gb Climate emulator structure
 * @param params HICMA parameters
 * @return Pointer to the initialized taskpool
 */
parsec_taskpool_t*
climate_emulator_geqsht_forward_pre_computed_version_New(parsec_context_t *parsec,
        climate_emulator_struct_t *gb,
        hicma_parsec_params_t *params)
{
    // Extract matrix descriptors from the climate emulator structure
    parsec_tiled_matrix_t *f_data = (parsec_tiled_matrix_t *)&gb->desc_f_data;
    parsec_tiled_matrix_t *flm = (parsec_tiled_matrix_t *)&gb->desc_flm;
    parsec_tiled_matrix_t *flmT = (parsec_tiled_matrix_t *)&gb->desc_flmT;
    parsec_tiled_matrix_t *Et1 = (parsec_tiled_matrix_t *)&gb->desc_Et1;
    parsec_tiled_matrix_t *Et2 = (parsec_tiled_matrix_t *)&gb->desc_Et2;
    parsec_tiled_matrix_t *Ep = (parsec_tiled_matrix_t *)&gb->desc_Ep;
    parsec_tiled_matrix_t *Slmn = (parsec_tiled_matrix_t *)&gb->desc_Slmn;
    parsec_tiled_matrix_t *Ie = (parsec_tiled_matrix_t *)&gb->desc_Ie;
    parsec_tiled_matrix_t *Io = (parsec_tiled_matrix_t *)&gb->desc_Io;
    parsec_tiled_matrix_t *P = (parsec_tiled_matrix_t *)&gb->desc_P;
    parsec_tiled_matrix_t *D = (parsec_tiled_matrix_t *)&gb->desc_D;

    // Create the forward geqsht taskpool with all matrix descriptors
    parsec_climate_emulator_geqsht_forward_pre_computed_version_taskpool_t
        *taskpool = parsec_climate_emulator_geqsht_forward_pre_computed_version_new(f_data, flm, flmT, Et1, Et2, Ep, Slmn, Ie,Io, P, D, gb);

    // Initialize memory pool for Gmtheta_r (complex double data)
    taskpool->_g_p_Gmtheta_r = (parsec_memory_pool_t*)malloc(sizeof(parsec_memory_pool_t));
    parsec_private_memory_init( taskpool->_g_p_Gmtheta_r, gb->f_data_M * gb->Ep_N * sizeof(complex double) );

    // Initialize memory pool for Fmnm (complex double data)
    taskpool->_g_p_Fmnm = (parsec_memory_pool_t*)malloc(sizeof(parsec_memory_pool_t));
    parsec_private_memory_init( taskpool->_g_p_Fmnm, gb->Et1_M * gb->Ep_N * sizeof(complex double) );

    // Initialize memory pool for work_1 (complex double data)
    taskpool->_g_p_work_1 = (parsec_memory_pool_t*)malloc(sizeof(parsec_memory_pool_t));
    parsec_private_memory_init( taskpool->_g_p_work_1, gb->Et2_M * gb->P_N * sizeof(complex double) );

    // Initialize memory pool for work_2 (complex double data)
    taskpool->_g_p_work_2 = (parsec_memory_pool_t*)malloc(sizeof(parsec_memory_pool_t));
    parsec_private_memory_init( taskpool->_g_p_work_2, gb->Et2_M * gb->Ep_N * sizeof(complex double) );

#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT) || defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
    int nb = 0, *dev_index;

    /** Find all available CUDA/HIP devices for GPU acceleration */
    hicma_parsec_find_cuda_devices( &dev_index, &nb);

    // Set GPU workspace and device information
    taskpool->_g_ws_gpu = (void *)gb->ws;
    taskpool->_g_nb_cuda_devices = nb;
    taskpool->_g_cuda_device_index = dev_index;
#endif

    // Calculate batch size based on time slots, nodes, and cores for load balancing
    taskpool->_g_batch = (int)(params->time_slots/params->nodes/params->cores) + 1; 

    // Configure the arena for f_data matrix (complex double precision)
    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_climate_emulator_geqsht_forward_pre_computed_version_f_data_ADT_IDX],
                            parsec_datatype_double_complex_t, PARSEC_MATRIX_FULL,
                            1, f_data->mb, f_data->nb, f_data->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    // Configure the arena for flm matrix (double precision)
    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_climate_emulator_geqsht_forward_pre_computed_version_flm_ADT_IDX],
                            parsec_datatype_double_t, PARSEC_MATRIX_FULL,
                            1, flm->mb, flm->nb, flm->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    return (parsec_taskpool_t*)taskpool;
}

/**
 * @brief Clean up and destroy the PaRSEC forward geqsht taskpool
 * 
 * This function properly deallocates all arenas, memory pools, and the taskpool itself
 * to prevent memory leaks.
 * 
 * @param taskpool Pointer to the taskpool to destroy
 */
void climate_emulator_geqsht_forward_pre_computed_version_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_climate_emulator_geqsht_forward_pre_computed_version_taskpool_t *climate_emulator_geqsht_forward_pre_computed_version_taskpool = (parsec_climate_emulator_geqsht_forward_pre_computed_version_taskpool_t *)taskpool;
    
    // Remove the arena for f_data matrix
    parsec_del2arena(&climate_emulator_geqsht_forward_pre_computed_version_taskpool->arenas_datatypes[PARSEC_climate_emulator_geqsht_forward_pre_computed_version_f_data_ADT_IDX]);
    
    // Remove the arena for flm matrix
    parsec_del2arena(&climate_emulator_geqsht_forward_pre_computed_version_taskpool->arenas_datatypes[PARSEC_climate_emulator_geqsht_forward_pre_computed_version_flm_ADT_IDX]);
    
    // Clean up all memory pools
    parsec_private_memory_fini( climate_emulator_geqsht_forward_pre_computed_version_taskpool->_g_p_Gmtheta_r);
    parsec_private_memory_fini( climate_emulator_geqsht_forward_pre_computed_version_taskpool->_g_p_Fmnm);
    parsec_private_memory_fini( climate_emulator_geqsht_forward_pre_computed_version_taskpool->_g_p_work_1);
    parsec_private_memory_fini( climate_emulator_geqsht_forward_pre_computed_version_taskpool->_g_p_work_2);
    
    // Free the taskpool itself
    parsec_taskpool_free(taskpool);
}

/**
 * @brief Main function to execute the forward geqsht operation
 * 
 * This function orchestrates the entire forward geqsht process:
 * 1. Creates and configures the forward geqsht taskpool
 * 2. Executes the forward transformations in parallel across all columns and batches
 * 3. Waits for completion and cleans up resources
 * 
 * @param parsec PaRSEC context
 * @param gb Climate emulator structure
 * @param params HICMA parameters
 * @return 0 on success
 */
int climate_emulator_geqsht_forward_pre_computed_version(parsec_context_t *parsec,
        climate_emulator_struct_t *gb,
        hicma_parsec_params_t *params)
{
    // Create the forward geqsht taskpool
    parsec_taskpool_t *parsec_climate_emulator_geqsht_forward_pre_computed_version = NULL;
    parsec_climate_emulator_geqsht_forward_pre_computed_version = climate_emulator_geqsht_forward_pre_computed_version_New( parsec, gb, params);
    
    if( parsec_climate_emulator_geqsht_forward_pre_computed_version != NULL ){
        // Add the taskpool to the PaRSEC context
        parsec_context_add_taskpool(parsec, parsec_climate_emulator_geqsht_forward_pre_computed_version);
        
        // Start execution of all forward transformation tasks
        parsec_context_start(parsec);
        
        // Wait for all tasks to complete
        parsec_context_wait(parsec);
        
        // Clean up the taskpool
        climate_emulator_geqsht_forward_pre_computed_version_Destruct(parsec_climate_emulator_geqsht_forward_pre_computed_version);
    }

    return 0;
}

%}
