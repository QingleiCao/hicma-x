extern "C" %{
/**
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2018-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2018-2023     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 **/
 * @precisions normal z -> s d c
 * $COPYRIGHT
 *
 */

/**
 * @file gemm_TN_batched.jdf
 * @brief JDF (Just Data Flow) implementation for batched General Matrix Multiply operation C = alpha * A^T * B + beta * C
 * 
 * This file implements a high-performance batched GEMM operation using PaRSEC runtime system.
 * The operation computes: C = alpha * A^T * B + beta * C where:
 * - A^T denotes the transpose of matrix A
 * - B is used as-is (no transpose)
 * - C is the result matrix
 * 
 * Key features:
 * - Supports mixed precision computation (int8 input, int32 intermediate, float output)
 * - GPU acceleration with CUDA/HIP support and batched operations
 * - Memory optimization with data type conversions
 * - Triangular matrix support (only processes upper triangular part)
 * - Task batching for improved GPU utilization
 */

#include "hicma_parsec.h"

/* Define the different shapes this JDF is using */
#define A_SHAPE 0  /**< Shape identifier for matrix A */
#define B_SHAPE 1  /**< Shape identifier for matrix B */
#define C_SHAPE 2  /**< Shape identifier for matrix C */

#include "gemm_TN.h"


void parsec_print_tile(float *A, int m, int n){
      printf("\n");
    for(int i=0;i<m;i++){
       for(int j=0;j<n;j++){
       //printf("[%d,%d]:(%f), ", i, j, tempaa[i*dcA->.super.llm+j]);
          printf("%f, ", A[j*m+i]);
       }
       printf("\n");
    }
}

void parsec_print_tile_int8(int8_t *A, int m, int n){
      printf("\n");
    for(int i=0;i<m;i++){
       for(int j=0;j<n;j++){
       //printf("[%d,%d]:(%f), ", i, j, tempaa[i*dcA->.super.llm+j]);
          //printf("%, " PRId8, A[j*m+i]);
          printf("%d ", A[j*m+i]);
       }
       printf("\n");
    }
}

void parsec_print_tile_int(int *A, int m, int n){
      printf("\n");
    for(int i=0;i<m;i++){
       for(int j=0;j<n;j++){
       //printf("[%d,%d]:(%f), ", i, j, tempaa[i*dcA->.super.llm+j]);
          //printf("%, " PRId8, A[j*m+i]);
          printf("%d ", A[j*m+i]);
       }
       printf("\n");
    }
}


static inline parsec_hook_return_t
always_here(const parsec_task_t* task)
{
    (void)task;
    return PARSEC_HOOK_RETURN_DONE;
}

static inline parsec_hook_return_t
never_here(const parsec_task_t* task)
{
    (void)task;
    return PARSEC_HOOK_RETURN_NEXT;
}

/**
 * @brief Helper structure for managing memory deallocation in batched operations
 * This structure stores pointers to dynamically allocated memory that needs to be freed
 * when the batched task completes.
 */
typedef struct deallocation_helper_s {
  parsec_assignment_t m;    /**< Row assignment for the task */
  parsec_assignment_t n;    /**< Column assignment for the task */
  parsec_assignment_t k;    /**< Panel assignment for the task */
  void** my_storage[6];     /**< Array of pointers to allocated memory blocks */
} free_helper_t;

/**
 * @brief Callback function for completing batched GPU tasks
 * This function is called when a batched GPU task completes and is responsible
 * for cleaning up dynamically allocated memory and managing task completion.
 * 
 * @param dev GPU device module
 * @param gpu_task Pointer to the GPU task being completed
 * @param gpu_stream GPU execution stream
 * @return PARSEC_HOOK_RETURN_DONE on success
 */
static int 
complete_batched_callback(parsec_device_gpu_module_t *dev,
                          parsec_gpu_task_t ** gpu_task,
                          parsec_gpu_exec_stream_t *gpu_stream)
{

    /*__parsec_gemm_TN_GEMM_task_t* task = (__parsec_gemm_TN_GEMM_task_t*)*gpu_task;*/

    // Get the deallocation helper from the task locals
    free_helper_t* helper = (free_helper_t*)(((*gpu_task)->ec)->locals);
     /*printf(" release %p and %p and %p\n", helper->my_storage[0], helper->my_storage[1], helper->my_storage[2]);*/

    // Free allocated memory
    void** ptr0=helper->my_storage[0];  /**< Host memory pointer array */
    void** ptr1=helper->my_storage[1];  /**< Device memory pointer array */
    if(ptr0!=NULL) free(ptr0);          /**< Free host memory */
    if(ptr1!=NULL) cudaFree(ptr1);      /**< Free device memory */

    PARSEC_DEBUG_VERBOSE((10, parsec_debug_output, "complete_batched_callback for batched task %p on stream %s{%p}\n",
                          gpu_task, gpu_stream->name, (void*)gpu_stream));
    (void)dev; (void) gpu_task; (void)gpu_stream;
    
    // Merge the completed task back into the output stream
    parsec_list_item_t* output_stream_ghost = &dev->exec_stream[1]->fifo_pending->ghost_element;
    parsec_list_item_ring_merge(output_stream_ghost, &(*gpu_task)->list_item);

    // Clean up task completion stage
    (*gpu_task)->complete_stage = NULL;
    *gpu_task = NULL;
    return PARSEC_HOOK_RETURN_DONE;
}

%}

/*
 * Global Parameters and Data Descriptors
 * =====================================
 */

/* Keep this first, as in all jdf in this directory, to
 * enable switching between GEMM implementations.
 */
gemm_type [ type = int hidden=on default="HICMA_GEMM_TN" ]  /**< GEMM implementation type identifier */

transA [type = int]  /**< Transpose flag for matrix A (1=transpose, 0=no transpose) */
transB [type = int]  /**< Transpose flag for matrix B (1=transpose, 0=no transpose) */

alpha  [type = float]  /**< Scaling factor for the matrix product A^T * B */
beta   [type = float]  /**< Scaling factor for the existing matrix C */

descA     [type = "const parsec_tiled_matrix_t*"]  /**< Descriptor for input matrix A */

// descB should be the same as descA !!!!
descB     [type = "const parsec_tiled_matrix_t*"]  /**< Descriptor for input matrix B (should be same as descA) */

descC     [type = "const parsec_tiled_matrix_t*"]  /**< Descriptor for output matrix C */

params_tlr   [ type = "hicma_parsec_params_t *" ]  /**< HiCMA PaRSEC parameters */

/* GPU workspace */
ws_gpu       [ type = "void *" hidden = on default = NULL ]  /**< GPU workspace for CUDA/HIP operations */

/*Workspace*/
p_work_int       [ type = "parsec_memory_pool_t *" hidden = on default = NULL ]  /**< Memory pool for integer workspace */

/* GPU number and index */
nb_cuda_devices      [ type = "int"   hidden = on default = 0 ]  /**< Number of available CUDA devices */
cuda_device_index    [ type = "int *" hidden = on default = "NULL"]  /**< Array of CUDA device indices */


/** We need to keep the set of handle-globals the same between
 *  gemm_*.jdf, to ensure that all handles can be destroyed
 *  with a generic function. P and Q are unused here. */
P      [type = "int" hidden=on default="-1"]  /**< Process grid rows (unused in this implementation) */
Q      [type = "int" hidden=on default="-1"]  /**< Process grid columns (unused in this implementation) */

/* Look ahead on both dimensions */
lookP  [type = "int" hidden=on default="dplasma_aux_getGEMMLookahead(descC)"]  /**< Look-ahead depth for P dimension */
lookQ  [type = "int" hidden=on default="dplasma_aux_getGEMMLookahead(descC)"]  /**< Look-ahead depth for Q dimension */

/**************************************************
 *               Define Arena                     *
 **************************************************/
my_arena(k)

// Execution space
k = 0 .. 0

// Parallel partitioning
:descC(0, 0)

// Parameters
READ A1 <- NULL       [ type = FULL_SP ]
READ A2 <- NULL       [ type = FULL_I8 ]
READ A3 <- NULL       [ type = FULL_I32 ]

BODY
{
    //fprintf(stderr, "lookP %d loopQ %d\n", lookP, lookQ);
}
END

/**************************************************
 *                       READ_A                   *
 **************************************************/
READ_A(k, m)  [profile = off]

k = 0 .. descA->mt-1
m = 0 .. descA->nt-1

: descA(k, m)

READ A <- descA(k, m)
       -> ((k!=descA->mt-1)) ? A GEMM(m, 0 .. m, k)  [ type_remote = FULL_I8 ]
       -> ((k==descA->mt-1)) ? A GEMM(m, 0 .. m, k)  [ type_remote = FULL_SP ] 
       -> ((k!=descB->mt-1)) ? B GEMM(m .. descC->mt-1, m, k)  [ type_remote = FULL_I8 ]
       -> ((k==descB->mt-1)) ? B GEMM(m .. descC->mt-1, m, k)  [ type_remote = FULL_SP ]

CTL ctla <- (k >= lookQ) ? ctla GEMM(m, 0 .. m, k-lookQ)
CTL ctlb <- (k >= lookP) ? ctlb GEMM(m .. descC->mt-1, m, k-lookP)

BODY
{
    printlog("rank %u <- A(%d,%d)\n", ((parsec_data_collection_t*)descA)->myrank, k, m);
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)  || defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
    if( nb_cuda_devices > 0 ) {
        int g = gpu_load_balance_2d( k, m, params_tlr );
        parsec_advise_data_on_device( _f_A->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
    }
#endif
}
END


/**************************************************
 *                       READ_C                   *
 **************************************************/
READ_C(m, n) [profile = off]

m = 0 .. descC->mt-1
n = 0 .. descC->nt-1

: descC(m, n)

READ C <- descC(m, n)
       -> C GEMM(m, n, 0)      [ type_remote = FULL_SP ]

BODY
{
    printlog("rank %u <- C(%d,%d)\n", ((parsec_data_collection_t*)descC)->myrank, m, n);
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)  || defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
    if( nb_cuda_devices > 0 ) {
        int g = gpu_load_balance_2d( m, n, params_tlr );
        parsec_advise_data_on_device( _f_C->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
    }
#endif
}
END



/**************************************************
 *                       GEMM                     *
 **************************************************/
GEMM(m, n, k)

// Execution space
m = 0 .. descC->mt-1
n = 0 .. m 
k = 0 .. descA->mt-1

// Parallel partitioning
: descC(m, n)

// Parameters
READ A <- ((k!=descA->mt-1)) ? A READ_A(k, m)         [ type_remote = FULL_I8 ]
       <- ((k==descA->mt-1)) ? A READ_A(k, m)         [ type_remote = FULL_SP ] 

READ B <- ((k!=descA->mt-1)) ? A READ_A(k, n)         [ type_remote = FULL_I8 ]
       <- ((k==descA->mt-1)) ? A READ_A(k, n)         [ type_remote = FULL_SP ]

RW   C <- (k == 0)             ? C READ_C(m, n)       [ type_remote = FULL_SP ]  
       <- (k != 0)             ? C GEMM( m, n, k-1 )  [ type_remote = FULL_I32 ] 
       -> (k == (descA->mt-1)) ? descC(m, n)       
       -> (k != (descA->mt-1)) ? C GEMM( m, n, k+1 )  [ type_remote = FULL_I32 ] 

CTL ctla -> (k < (descA->mt-lookQ)) ? ctla READ_A(k+lookQ, m)
CTL ctlb -> (k < (descA->mt-lookP)) ? ctlb READ_A(k+lookP, n)


//BODY [type=CUDA]
//{
//    if (m < n) {
//        return PARSEC_HOOK_RETURN_DONE;
//        printf("m: %d n: %d\n", m, n);
//    }
//    
//    float lalpha = alpha;
//    float lbeta  = (k == 0) ? beta : 1.0;
//
//    int tempmm = m == descC->mt-1 ? descC->m - m * descC->mb : descC->mb;
//    int tempnn = n == descC->nt-1 ? descC->n - n * descC->nb : descC->nb;
//    int tempkk = k == descA->mt-1 ? descA->m - k * descA->mb : descA->mb;
//    int ldak = descA->mb;
//    int ldbk = descB->mb;
//    int ldcm = descC->mb;
//
//    // Get handle_cublas 
//    parsec_potrf_workspace_t *_ws_gpu = (parsec_potrf_workspace_t *)ws_gpu;
//    parsec_potrf_stream_workspace_t *stream_found = lookup_gpu_workspace(cuda_device, cuda_stream, _ws_gpu);
//    cublasHandle_t handle = stream_found->handle_cublas;
//    cublasSetStream( handle, cuda_stream->cuda_stream );
//    cublasStatus_t status;
//
//    int ialpha=(int)lalpha;
//    int ibeta=(int)lbeta;
//#if 1
//    if(k == 0) float_2int_array_unary(tempmm, tempnn, C, ldcm, cuda_stream->cuda_stream);
//    if (k != descA->mt-1) {
//        status = cublasGemmEx(handle, CUBLAS_OP_T, CUBLAS_OP_N,
//                tempmm, tempnn, tempkk,
//                &ialpha, A, CUDA_R_8I, ldak,
//                B, CUDA_R_8I, ldbk,
//                &ibeta, C, CUDA_R_32I, ldcm,
//                CUDA_R_32I, CUBLAS_GEMM_DEFAULT); 
//        //printf("\n k:%d %s %d\n", k,  __FILE__, __LINE__);                    
//    } else {
//        // parsec_print_tile(A, tempmm, tempmm);
//        int_2float_array_unary(tempmm, tempnn, C, ldcm, cuda_stream->cuda_stream);
//        status = cublasSgemm( handle, CUBLAS_OP_T, CUBLAS_OP_N,
//                tempmm, tempnn, tempkk,
//                &lalpha, A, ldak,
//                B, ldbk,
//                &lbeta,  C, ldcm ); 
//    }
//#endif
//
//}


BODY [type=CUDA
      batch = true]
{
    if (m < n) {
        return PARSEC_HOOK_RETURN_DONE;
        printf("m: %d n: %d\n", m, n);
    }

    // Print the process
    if(0 == m && 0 == n) hicma_parsec_print_process_syrk(descA->mt, k, params_tlr->start_time_syrk);

    int tempmm = m == descC->mt-1 ? descC->m - m * descC->mb : descC->mb;
    int tempnn = n == descC->nt-1 ? descC->n - n * descC->nb : descC->nb;
    int tempkk = k == descA->mt-1 ? descA->m - k * descA->mb : descA->mb;
    int ldak = descA->mb;
    int ldbk = descB->mb;
    int ldcm = descC->mb;

    float lalpha = alpha;
    float lbeta  = (k == 0) ? beta : 1.0;
    int ialpha=(int)lalpha;
    int ibeta=(int)lbeta;

    const int max_batch_count=50;


    //Let make batchable tasks
    parsec_list_item_singleton(&gpu_task->list_item);
    parsec_list_item_t* store_back = NULL;
    int batch_count = 1;  /* we start with the current gpu_task */
    bool do_sgemm=(k==descA->mt-1);
    if( !parsec_list_nolock_is_empty(gpu_stream->fifo_pending) ) 
    {  /* more than one gpu_task in the ring */
        do 
        {
            parsec_list_item_t* item = parsec_list_pop_front(gpu_stream->fifo_pending);
            parsec_list_item_singleton(item);
            parsec_gpu_task_t* task = (parsec_gpu_task_t*)item;



            __parsec_gemm_TN_GEMM_task_t *other_task = (__parsec_gemm_TN_GEMM_task_t *)task->ec;
            __parsec_gemm_TN_GEMM_task_t *my_task = (__parsec_gemm_TN_GEMM_task_t *)gpu_task->ec;


            const int m_other=other_task->locals.m.value;
            const int n_other=other_task->locals.n.value;
            const int k_other=other_task->locals.k.value;
            const int tempmm_other = m == descC->mt-1 ? descC->m - m_other * descC->mb : descC->mb;
            const int tempnn_other = n == descC->nt-1 ? descC->n - n_other * descC->nb : descC->nb;
            const int tempkk_other = k == descA->mt-1 ? descA->m - k_other * descA->mb : descA->mb;
            bool do_sgemm_other=(k_other==descA->mt-1);

            bool is_batchable=
                 ( other_task->task_class == my_task->task_class ) &&
                 ( tempmm == tempmm_other ) &&
                 ( tempnn == tempnn_other ) &&
                 ( tempkk == tempkk_other ) &&
                 ( do_sgemm == do_sgemm_other );
            if(k==0 && k_other!=0) is_batchable=false;
                 

            if(is_batchable) 
            {

                /*printf("Batchable. Other_task sm: %d, sn: %d, sk:%d, do_sgemm: %d,m: %d, n: %d, k:%d, my_task: sm:%d,sn: %d, sk: %d, do_sgemm: %d, m: %d,n: %d, k: %d\n", */
                        /*tempmm, tempnn, tempkk,(int) do_sgemm,m,n,k,*/
                        /*tempmm_other, tempnn_other, tempkk_other,(int)do_sgemm_other, m_other, n_other, k_other);*/
                /* same task class as the current one, possible to batch */
                (void)parsec_list_item_ring_push(&gpu_task->list_item, (parsec_list_item_t*)task);
                batch_count++;  /* one more into the batch */
                PARSEC_DEBUG_VERBOSE((10, parsec_debug_output, "Add task %p to the %p batch on stream %s{%p}\n", task, gpu_task, gpu_stream->name, (void*)gpu_stream));
                if( max_batch_count == batch_count ) 
                {
                    /* let's stop here for now */
                    break;
                }
            } 
            else 
            {
                /*printf("Not batchable. Other_task sm: %d, sn: %d, sk:%d, do_sgemm: %d,m: %d, n: %d, k:%d, my_task: sm:%d,sn: %d, sk: %d, do_sgemm: %d, m: %d,n: %d, k: %d\n", */
                        /*tempmm, tempnn, tempkk,(int) do_sgemm,m,n,k,*/
                        /*tempmm_other, tempnn_other, tempkk_other,(int)do_sgemm_other, m_other, n_other, k_other);*/
                if( NULL == store_back ) 
                {
                    store_back = item;
                } 
                else 
                {
                    parsec_list_item_ring_push(store_back, item);  /* build the list of un-batcheable tasks */
                }
            }
        } while( !parsec_list_nolock_is_empty(gpu_stream->fifo_pending) );
         /*we now have two separated task rings: the gpu_task with all the tasks that will be batched*/
         /*and the ring that has all the remaining items in the list (including the list's ghost_elem).*/
         /*The remaining list is already stored in the gpu_stream->fifo_pending.*/
        if(batch_count > 1) gpu_task->complete_stage = complete_batched_callback;
        PARSEC_DEBUG_VERBOSE((10, parsec_debug_output, "submit multiple tasks into one %p on stream %s{%p}\n",
                              gpu_task, gpu_stream->name, (void*)gpu_stream));
        if( NULL != store_back ) 
        {
            /*parsec_list_push_back(gpu_stream->fifo_pending, store_back);*/
            parsec_list_item_ring_merge(&gpu_stream->fifo_pending->ghost_element, store_back);
        }
    }

    


    // Get handle_cublas 
    parsec_potrf_workspace_t *_ws_gpu = (parsec_potrf_workspace_t *)ws_gpu;
    parsec_potrf_stream_workspace_t *stream_found = lookup_gpu_workspace(cuda_device, cuda_stream, _ws_gpu);
    cublasHandle_t handle = stream_found->handle_cublas;
    cublasSetStream( handle, cuda_stream->cuda_stream );
    cublasStatus_t status;

    if(batch_count==1)
    {

        parsec_gpu_task_t* current_gpu_task = gpu_task;
        do 
        {
            /*__parsec_stage_custom_TASK_GPU_task_t *task = (__parsec_stage_custom_TASK_GPU_task_t*)current_gpu_task->ec;*/

            __parsec_gemm_TN_GEMM_task_t *task = (__parsec_gemm_TN_GEMM_task_t*)current_gpu_task->ec;

            _f_C = task->data._f_C.data_out;
            C = PARSEC_DATA_COPY_GET_PTR(_f_C);
            _f_A = task->data._f_A.data_out;
            A = PARSEC_DATA_COPY_GET_PTR(_f_A);
            _f_B = task->data._f_B.data_out;
            B = PARSEC_DATA_COPY_GET_PTR(_f_B);

            const int k_task=task->locals.k.value;
            if(k_task == 0) float_2int_array_unary(tempmm, tempnn, C, ldcm, cuda_stream->cuda_stream);

            if (!do_sgemm) 
            {
                status = cublasGemmEx(handle, CUBLAS_OP_T, CUBLAS_OP_N,
                        tempmm, tempnn, tempkk,
                        &ialpha, A, CUDA_R_8I, ldak,
                        B, CUDA_R_8I, ldbk,
                        &ibeta, C, CUDA_R_32I, ldcm,
                        CUDA_R_32I, CUBLAS_GEMM_DEFAULT); 
            } 
            else 
            {
                // parsec_print_tile(A, tempmm, tempmm);
                int_2float_array_unary(tempmm, tempnn,C , ldcm, cuda_stream->cuda_stream);
                status = cublasSgemm( handle, CUBLAS_OP_T, CUBLAS_OP_N,
                        tempmm, tempnn, tempkk,
                        &lalpha, A, ldak,
                        B, ldbk,
                        &lbeta,  C, ldcm ); 
            }
            current_gpu_task = (parsec_gpu_task_t*)current_gpu_task->list_item.list_next;
        } while( current_gpu_task != gpu_task );
    }
    else
    {


        void** ptr=(void**)malloc(3*batch_count*sizeof(void*));
        void** Aarray=ptr+( 0*batch_count );
        void** Barray=ptr+( 1*batch_count );
        void** Carray=ptr+( 2*batch_count );
        __parsec_gemm_TN_GEMM_task_t *gmy_task = (__parsec_gemm_TN_GEMM_task_t *)gpu_task->ec;
        ((free_helper_t*) &gmy_task->locals)->my_storage[0]=ptr;

        //Allocate device memory for array of pointers
        void** d_ptr=NULL;
        cudaMalloc((void**)d_ptr, 3*batch_count * sizeof(void*));
        ((free_helper_t*) &gmy_task->locals)->my_storage[1]=d_ptr;
        void** d_Aarray=d_ptr+(0*batch_count);
        void** d_Barray=d_ptr+(1*batch_count);
        void** d_Carray=d_ptr+(2*batch_count);

        //Here we just iterate over the task 
        parsec_gpu_task_t* current_gpu_task = gpu_task;
        if(do_sgemm)
        {

            //iterate through tasks
            //TODO: batch it
            int count=0;
            do 
            {
                __parsec_gemm_TN_GEMM_task_t *task = (__parsec_gemm_TN_GEMM_task_t*)current_gpu_task->ec;

                A = PARSEC_DATA_COPY_GET_PTR(task->data._f_A.data_out);
                B = PARSEC_DATA_COPY_GET_PTR(task->data._f_B.data_out);
                C = PARSEC_DATA_COPY_GET_PTR(task->data._f_C.data_out);


                const int k_task=task->locals.k.value;
                if(k_task == 0) float_2int_array_unary(tempmm, tempnn, C, ldcm, cuda_stream->cuda_stream);

                int_2float_array_unary(tempmm, tempnn,C , ldcm, cuda_stream->cuda_stream);

                Aarray[count]=A;
                Barray[count]=B;
                Carray[count]=C;

                /*status = cublasSgemm( handle, CUBLAS_OP_T, CUBLAS_OP_N,*/
                        /*tempmm, tempnn, tempkk,*/
                        /*&lalpha, A, ldak,*/
                        /*B, ldbk,*/
                        /*&lbeta,  C, ldcm ); */
                current_gpu_task = (parsec_gpu_task_t*)current_gpu_task->list_item.list_next;

                ++count;
            } while( current_gpu_task != gpu_task );


                cudaMemcpyAsync(d_ptr, ptr, 3*count * sizeof(void*), cudaMemcpyHostToDevice, cuda_stream->cuda_stream);
                status = cublasSgemmBatched( handle, CUBLAS_OP_T, CUBLAS_OP_N,
                        tempmm, tempnn, tempkk,
                        &lalpha, (const void**)d_Aarray, ldak,
                        (const void**)d_Barray, ldbk,
                        &lbeta,  d_Carray, ldcm,count ); 

        }
        else  //Here we call the batched version
        {

            //Accumulate the ptrs: Note this could likely also be done directly above
            int count=0;
            do 
            {

                __parsec_gemm_TN_GEMM_task_t *task = (__parsec_gemm_TN_GEMM_task_t*)current_gpu_task->ec;
                A = PARSEC_DATA_COPY_GET_PTR(task->data._f_A.data_out);
                B = PARSEC_DATA_COPY_GET_PTR(task->data._f_B.data_out);
                C = PARSEC_DATA_COPY_GET_PTR(task->data._f_C.data_out);

                const int k_task=task->locals.k.value;
                if(k_task == 0) float_2int_array_unary(tempmm, tempnn, C, ldcm, cuda_stream->cuda_stream);

                Aarray[count]=A;
                Barray[count]=B;
                Carray[count]=C;

                ++count;
                current_gpu_task = (parsec_gpu_task_t*)current_gpu_task->list_item.list_next;
            } while( current_gpu_task != gpu_task );

            //Memcpy the array ointers
            cudaMemcpyAsync(d_ptr, ptr, 3*count * sizeof(void*), cudaMemcpyHostToDevice, cuda_stream->cuda_stream);
            status = cublasGemmBatchedEx(handle, CUBLAS_OP_T, CUBLAS_OP_N,
                    tempmm, tempnn, tempkk,
                    &ialpha, (const void**)d_Aarray, CUDA_R_8I, ldak,
                    (const void**)d_Barray, CUDA_R_8I, ldbk,
                    &ibeta, d_Carray, CUDA_R_32I, ldcm, count,
                    CUDA_R_32I, CUBLAS_GEMM_DEFAULT); 
        }

    }



}
END
