extern "C" %{
/**
 * @file matrix_ssum.jdf
 * @brief Matrix sum computation for sparse matrices with rank information
 * 
 * This file implements a PaRSEC task for computing the sum of all elements
 * in a sparse matrix that uses low-rank representations. The computation
 * handles both dense tiles and low-rank tiles (stored as U*V^T factorization).
 * 
 * The algorithm processes lower triangular matrices and accumulates the sum
 * across all tiles, with special handling for low-rank tiles where only
 * the rank information is stored in the Ar matrix.
 * 
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2018-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2018-2023     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 **/

#include "hicma_parsec.h"

/**
 * @brief Core function to compute sum of matrix elements with NaN/Inf detection
 * 
 * This function computes the sum of all elements in a matrix tile while
 * detecting and reporting NaN and Inf values for debugging purposes.
 * 
 * @param [in] A:    Pointer to the matrix data
 * @param [in] mb:   Number of rows in the tile
 * @param [in] nb:   Number of columns in the tile
 * @param [in] lda:  Leading dimension of the matrix
 * @param [in] m:    Row index of the tile (for error reporting)
 * @param [in] n:    Column index of the tile (for error reporting)
 * @return Sum of all elements in the tile
 */
static float parsec_matrix_sum_CORE (float *A, int mb, int nb, int lda, int m, int n) {
    float res = 0.0;
    int indicator_nan = 0;  // Flag to track NaN values
    int indicator_inf = 0;  // Flag to track Inf values
    
    // Iterate through all elements in the tile
    for (int j = 0; j < nb; j++) {
        for (int i = 0; i < mb; i++) {
            // Check for NaN values
            if( isnan(A[j*lda+i]) ) {
                indicator_nan = 1;
            }
            // Check for Inf values
            if( isinf(A[j*lda+i]) ) {
                indicator_inf = 1;
            }
            // Accumulate the sum
            res += A[j*lda+i]; 
        }
    }

    // Report NaN values if found
    if( 1 == indicator_nan ) {
        fprintf(stderr, "***** NAN ****** in ( %d %d )\n", m, n);
    }

    // Report Inf values if found
    if( 1 == indicator_inf ) {
        fprintf(stderr, "***** INF ****** in ( %d %d )\n", m, n);
    }

	return res;
}

%}

/* Global parameters for the matrix sum computation task
 * descA:     The main matrix descriptor containing tile data
 * descAr:    The rank matrix descriptor (aligned with descA) containing rank information
 * decisions: Array indicating the storage format for each tile (dense vs low-rank)
 * sum:       Array to store per-thread sum contributions
 */
descA            [ type = "parsec_tiled_matrix_t *" ]
descAr           [ type = "parsec_tiled_matrix_t *" aligned = descA ]
decisions        [ type = "uint16_t *" ]
sum              [ type = "float *" ]

/**************************************************
 *        Read and compute sum for each tile      *
 **************************************************/
Read(m, n)

// Execution space: process lower triangular tiles
m = 0 .. descA->lmt-1
n = 0 .. m 

// Parallel partitioning: each tile is processed by one task
: descA(m, n)

// Parameters: read both matrix data and rank information
READ A <- descA(m, n)
READ Ar <- descAr(m, n)

// Output: create a new data structure to store the tile sum
WRITE D <- NEW
        -> D Sum(m, n)

BODY
{
    // Determine the effective number of columns based on storage format
    int nb = descA->mb;  // Default: full tile size
    
    // For low-rank tiles, adjust the column count based on rank
    if( LOW_RANK_DP == decisions[n*descA->lmt+m] || LOW_RANK_SP == decisions[n*descA->lmt+m] )
        nb = ((int *)Ar)[0] * 2;  // Rank * 2 (for U and V factors)
    
    // Compute the sum of elements in this tile
    *((float *)D) = parsec_matrix_sum_CORE( A, descA->mb, nb, descA->mb, m, n );
    
    // Debug output (commented out for performance)
    //printf("%d %d : %lf\n", m, n, *((float *)D));
}
END

/**************************************************
 *        Accumulate sums across tiles            *
 **************************************************/
Sum(m, n)

// Execution space: process all tiles that have been computed
m = 0 .. descA->lmt-1
n = 0 .. m 

// Parallel partitioning: all sum tasks run on the same node
: descA(0, 0)

// Parameters: read the sum computed by the Read task
READ D <- D Read(m, n) 

BODY
{
    // Get thread ID for per-thread accumulation
	int tid = es->th_id;
	
	// Accumulate the tile sum into the thread's total
	sum[tid] += *((float *)D); 
}
END


extern "C" %{

/**
 * @brief Create a new matrix sum computation taskpool
 * 
 * This function creates and initializes a PaRSEC taskpool for computing
 * the sum of all elements in a sparse matrix with low-rank representations.
 * The taskpool handles both dense and low-rank tiles appropriately.
 * 
 * @param [in] dcA:       The main matrix descriptor, already distributed and allocated
 * @param [in] dcAr:      The rank matrix descriptor, aligned with dcA
 * @param [in] decisions: Array indicating storage format for each tile
 * @param [in] sum:       Array to store per-thread sum contributions
 * @return the parsec object to schedule, or NULL on error
 */
static parsec_taskpool_t*
parsec_matrix_ssum_New(parsec_tiled_matrix_t *dcA,
        parsec_tiled_matrix_t *dcAr,
        uint16_t *decisions,
        float *sum) 
{
    parsec_taskpool_t* matrix_ssum_taskpool;
    parsec_matrix_ssum_taskpool_t* taskpool = NULL;

    // Create the taskpool with the specified parameters
    taskpool = parsec_matrix_ssum_new(dcA, dcAr, decisions, sum); 
    matrix_ssum_taskpool = (parsec_taskpool_t*)taskpool;

    // Set up the arena for float data types
    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_matrix_ssum_DEFAULT_ADT_IDX],
                            parsec_datatype_float_t, PARSEC_MATRIX_FULL,
                            1, 1, 1, 1,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    return matrix_ssum_taskpool;
}

/**
 * @brief Destroy the matrix sum computation taskpool
 * 
 * @param [inout] taskpool: The PaRSEC taskpool to destroy
 */
static void parsec_matrix_ssum_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_matrix_ssum_taskpool_t *matrix_ssum_taskpool = (parsec_matrix_ssum_taskpool_t *)taskpool;
    
    // Clean up the arena
    parsec_del2arena(&matrix_ssum_taskpool->arenas_datatypes[PARSEC_matrix_ssum_DEFAULT_ADT_IDX]);
    
    // Free the taskpool
    parsec_taskpool_free(taskpool);
}

/**
 * @brief Compute the sum of all elements in a sparse matrix
 * 
 * This function computes the sum of all elements in a sparse matrix that uses
 * low-rank representations. It handles both dense tiles and low-rank tiles
 * (stored as U*V^T factorization) and performs the computation in parallel
 * across multiple threads and processes.
 * 
 * @param [in] parsec:     PaRSEC context for task scheduling
 * @param [in] dcA:        The main matrix descriptor, already distributed and allocated
 * @param [in] dcAr:       The rank matrix descriptor, aligned with dcA
 * @param [in] decisions:  Array indicating storage format for each tile
 * @return The sum of all elements in the matrix
 */
float hicma_parsec_matrix_ssum(parsec_context_t *parsec,
                         parsec_tiled_matrix_t *dcA,
                         parsec_tiled_matrix_t *dcAr,
                         uint16_t *decisions )
{
    parsec_taskpool_t *parsec_matrix_ssum = NULL;

    // Get number of threads for per-thread accumulation
    int nb_threads = parsec->virtual_processes[0]->nb_cores;
    float *sum = (float *)calloc( nb_threads, sizeof(float) );

    // Create and execute the sum computation taskpool
    parsec_matrix_ssum = parsec_matrix_ssum_New(dcA, dcAr, decisions, sum); 

    if( parsec_matrix_ssum != NULL ){
        parsec_context_add_taskpool(parsec, parsec_matrix_ssum);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);
        parsec_matrix_ssum_Destruct(parsec_matrix_ssum);
    }

    // Reduce per-thread contributions to get process-level sum
    float total = 0.0; 
    int root = dcA->super.rank_of(&dcA->super, 0, 0);
    if( dcA->super.myrank == root ) {
        for( int i = 0; i < nb_threads; i++ )
            total += sum[i];
    }

    // Broadcast the result to all processes
    MPI_Bcast( &total, 1, MPI_FLOAT, root, MPI_COMM_WORLD );

    // Clean up temporary storage
    free( sum );

    return total;
}

%}
