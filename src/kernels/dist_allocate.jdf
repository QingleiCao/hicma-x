extern "C" %{
/**
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2018-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2018-2023     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 **/
#include "hicma_parsec.h"


%}

// PaRSEC task parameters
descA         [ type = "parsec_tiled_matrix_t*" ]  // Matrix descriptor
params_tlr      [ type = "hicma_parsec_params_t *" ] // HICMA parameters

// Task definition for distributed allocation
dist_allocate(m, n)

// Task iteration space: full matrix
m = 0 .. descA->mt-1
n = 0 .. descA->nt-1

: descA(m, n)

// Data dependencies: read-write access to matrix tile
RW D <- descA(m, n)  
     ->descA(m, n)
READ D1 <- NULL           

BODY
{

    // Calculate actual tile dimensions (handle edge cases)
    int tempmm, tempnn, ldam, ldbm;
    int i, j;
    tempmm = ((m)==((descA->mt)-1)) ? ((descA->m)-(m*(descA->mb))) : (descA->mb);
    tempnn = ((n)==((descA->nt)-1)) ? ((descA->n)-(n*(descA->nb))) : (descA->nb);
    ldam = BLKLDD( descA, m );

    // Allocate memory based on GPU availability and data type
    if( params_tlr->gpus > 0 ) { 
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
        // CUDA pinned memory allocation
        #if GENOMICS_ALLOCATE_INT
            cudaMallocHost((void**)&this_task->data._f_D.data_out->device_private, descA->mb * descA->mb * sizeof(int32_t));
        #else
            cudaMallocHost((void**)&this_task->data._f_D.data_out->device_private, descA->mb * descA->mb * sizeof(DATATYPE));
        #endif
#endif
#if defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
        // HIP pinned memory allocation
        #if GENOMICS_ALLOCATE_INT
            hipHostMalloc((void**)&this_task->data._f_D.data_out->device_private, descA->mb * descA->mb * sizeof(int32_t), hipHostMallocDefault);
        #else       
            hipHostMalloc((void**)&this_task->data._f_D.data_out->device_private, descA->mb * descA->mb * sizeof(DATATYPE), hipHostMallocDefault);
        #endif
        // Note: hipHostMallocNumaUser could be used for NUMA-aware allocation
        // References:
        // https://github.com/ROCm-Developer-Tools/HIP/issues/2475
        // https://rocm-developer-tools.github.io/HIP/group__GlobalDefs.html
#endif
    } else {
        // CPU memory allocation
        #if GENOMICS_ALLOCATE_INT
            this_task->data._f_D.data_out->device_private = calloc(descA->mb * descA->mb, sizeof(int32_t));
        #else
            this_task->data._f_D.data_out->device_private = calloc(descA->mb * descA->mb, sizeof(DATATYPE));
        #endif
    }
        
}
END

extern "C" %{

/**
 * @brief Create a new PaRSEC taskpool for distributed allocation
 * 
 * @param A Matrix descriptor for the distributed matrix
 * @param params_tlr HICMA parameters
 * @return PaRSEC taskpool object for scheduling
 */
parsec_taskpool_t*
parsec_dist_allocate_New(parsec_tiled_matrix_t *A, 
                        hicma_parsec_params_t *params_tlr) 
{
    parsec_taskpool_t* band_dist_allocate_taskpool;
    parsec_dist_allocate_taskpool_t* taskpool = NULL;

    // Create the distributed allocate taskpool
    taskpool = parsec_dist_allocate_new(A, params_tlr);
    band_dist_allocate_taskpool = (parsec_taskpool_t*)taskpool;

    // Add appropriate datatype to arena based on compilation flags
    #if GENOMICS_ALLOCATE_INT
        parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_dist_allocate_DEFAULT_ADT_IDX],
                            parsec_datatype_int32_t, PARSEC_MATRIX_FULL,
                            1, A->mb, A->nb, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );
    #else
        parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_dist_allocate_DEFAULT_ADT_IDX],
                            parsec_datatype_float_t, PARSEC_MATRIX_FULL,
                            1, A->mb, A->nb, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );
    #endif

    return band_dist_allocate_taskpool;
}

/**
 * @brief Destroy the distributed allocate taskpool and free resources
 * 
 * @param taskpool PaRSEC taskpool to destroy
 */
void parsec_dist_allocate_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_dist_allocate_taskpool_t *dist_allocate_taskpool = (parsec_dist_allocate_taskpool_t *)taskpool;
    // Remove datatype from arena
    parsec_del2arena(&dist_allocate_taskpool->arenas_datatypes[PARSEC_dist_allocate_DEFAULT_ADT_IDX]);
    // Free the taskpool
    parsec_taskpool_free(taskpool);
}

/**
 * @brief Main function to perform distributed allocation
 * 
 * @param parsec PaRSEC context
 * @param A Matrix descriptor for the distributed matrix
 * @param params_tlr HICMA parameters
 * @return 0 on success
 */
int parsec_dist_allocate(parsec_context_t *parsec,
                         parsec_tiled_matrix_t *A, 
                         hicma_parsec_params_t *params_tlr) 
{
    parsec_taskpool_t *parsec_dist_allocate = NULL;

    // Create the distributed allocate taskpool
    parsec_dist_allocate = parsec_dist_allocate_New(A, params_tlr);

    if( parsec_dist_allocate != NULL ){
        // Add taskpool to context and execute
        parsec_context_add_taskpool(parsec, parsec_dist_allocate);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);
        // Clean up resources
        parsec_dist_allocate_Destruct(parsec_dist_allocate);
    }

    return 0;
}

%}
