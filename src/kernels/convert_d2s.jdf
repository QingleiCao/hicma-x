extern "C" %{
/*
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2018-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2018-2023     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 */

#include "hicma_parsec.h"

%}

// Matrix descriptor for the main matrix (double precision input)
descA         [ type = "parsec_tiled_matrix_t*" ]
// Matrix descriptor for rank information (aligned with descA)
descAr        [ type = "parsec_tiled_matrix_t*" aligned = descA ]
// Parameters for TLR (Tile Low Rank) operations
params_tlr    [ type = "hicma_parsec_params_t *" ]

// Task definition for converting double precision to single precision
Convert_d2s(m, n)

// Iterate over lower triangular matrix tiles only
m = 0 .. descA->lmt-1
n = 0 .. m

: descA(m, n)

// Conditional read-write access based on decision matrix
// Only process tiles that need conversion to single precision
RW A <- (%{ return DENSE_SP == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return DENSE_HP == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return DENSE_FP8 == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return LOW_RANK_SP == params_tlr->decisions[n*descA->lmt+m]; %})? descA(m, n): NULL        
     -> (%{ return DENSE_SP == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return DENSE_HP == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return DENSE_FP8 == params_tlr->decisions[n*descA->lmt+m]; %} || %{ return LOW_RANK_SP == params_tlr->decisions[n*descA->lmt+m]; %})? descA(m, n)              

// Read rank information for low-rank tiles
READ Ar <- (%{ return LOW_RANK_SP == params_tlr->decisions[n*descA->lmt+m]; %})? descAr(m, n): NULL     [ type  = AR ]

// Define remote arena types for different data formats
READ A1 <- NULL       [ type_remote = FULL_DP ]  // Full double precision
READ A2 <- NULL       [ type_remote = FULL_SP ]  // Full single precision  
READ A3 <- NULL       [ type_remote = UV_DP ]    // Low-rank U*V double precision
READ A4 <- NULL       [ type_remote = UV_SP ]    // Low-rank U*V single precision

BODY
{
    // Handle dense matrix tiles that need conversion to single precision
    if( DENSE_SP == params_tlr->decisions[n*descA->lmt+m]
            || DENSE_HP == params_tlr->decisions[n*descA->lmt+m]
            || DENSE_FP8 == params_tlr->decisions[n*descA->lmt+m] ) {

        int size = descA->nb;

        // Check if tile is already allocated in single precision
        if( IS_ALLOCATE_DP(m, n) ) { 
            if(0) printf("(%d %d) needs D2S\n", m, n);
        } else {
            // Already in single precision, just update element count
            this_task->data._f_A.data_out->original->nb_elts = descA->mb * size * sizeof(float);
            return PARSEC_HOOK_RETURN_DONE; 
        }

        // Perform conversion based on memory management strategy
        if( params_tlr->check || !params_tlr->adaptive_memory ) {
            // In-place conversion (unary operation)
            convert_d2s_unary_CPU(A, descA->mb, size);
        } else {
            // Allocate new memory and convert to single precision (binary operation)
            float *new_memory;
            
            // Allocate memory based on available GPU support
            if( params_tlr->gpus > 0 ) {
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
                cudaMallocHost((void**)&new_memory, descA->mb * size * sizeof(float));
#endif

#if defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
                hipHostMalloc((void**)&new_memory, descA->mb * size * sizeof(float), hipHostMallocDefault);
#endif
            } else {
                new_memory = malloc( descA->mb * size * sizeof(float) );
            }
            
            // Convert from double to single precision
            convert_d2s_binary_CPU(new_memory, this_task->data._f_A.data_out->device_private, descA->mb, size);

            // Free previous double precision memory and attach new single precision memory
            if( params_tlr->gpus > 0 ) {
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT) || defined(PARSEC_HAVE_DEV_HIP_SUPPORT)
                cudaFreeHost( this_task->data._f_A.data_out->device_private );
#endif
            } else {
                free( this_task->data._f_A.data_out->device_private );
            }
            this_task->data._f_A.data_out->device_private = new_memory;
        }
        // Update element count for single precision
        this_task->data._f_A.data_out->original->nb_elts = descA->mb * size * sizeof(float);
    }

    // Handle low-rank matrix tiles that need conversion to single precision
    if( LOW_RANK_SP == params_tlr->decisions[n*descA->lmt+m] ) { 
        // Calculate size based on rank (U*V format: rank * 2)
        int size = ((int *)Ar)[0] * 2;
        
        // Low rank does not support !adaptive_memory because of reallocation in Cholesky
        {
            // Allocate new memory and convert low-rank tile to single precision
            float *new_memory = malloc( descA->mb * size * sizeof(float) );
            convert_d2s_binary_CPU(new_memory, this_task->data._f_A.data_out->device_private, descA->mb, size);

            // Free previous memory and re-attach new memory
            free( this_task->data._f_A.data_out->device_private );
            this_task->data._f_A.data_out->original->nb_elts = descA->mb * size * sizeof(float);
            this_task->data._f_A.data_out->device_private = new_memory;
        }
    }
}
END

extern "C" %{

/**
 * @param [in] data:    the data, already distributed and allocated
 * @return the parsec object to schedule.
 */
parsec_taskpool_t*
hicma_parsec_convert_d2s_New(parsec_context_t *parsec,
        hicma_parsec_data_t *data,
        hicma_parsec_params_t *params_tlr) 
{
    if( !params_tlr->check && params_tlr->adaptive_memory ) {
        VERBOSE_PRINT(params_tlr->rank, params_tlr->verbose, (YEL"Convert DP to SP and reallocate in SP\n"RESET));
    }

    parsec_tiled_matrix_t *A = (parsec_tiled_matrix_t *)&data->dcA;
    if( params_tlr->band_size_dense >= params_tlr->NT && params_tlr->auto_band == 0 && !params_tlr->adaptive_memory ) {
        A = (parsec_tiled_matrix_t *)&data->dcAd;
    }
    parsec_tiled_matrix_t *Ar = (parsec_tiled_matrix_t *)&data->dcAr;

    parsec_taskpool_t* convert_d2s_taskpool;
    parsec_convert_d2s_taskpool_t* taskpool = NULL;

    taskpool = parsec_convert_d2s_new(A, Ar, params_tlr);
    convert_d2s_taskpool = (parsec_taskpool_t*)taskpool;

    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_convert_d2s_FULL_DP_ADT_IDX],
                            parsec_datatype_double_t, PARSEC_MATRIX_FULL,
                            1, A->mb, A->nb, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_convert_d2s_FULL_SP_ADT_IDX],
                            parsec_datatype_float_t, PARSEC_MATRIX_FULL,
                            1, A->mb, A->nb, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_convert_d2s_UV_DP_ADT_IDX],
                            parsec_datatype_double_t, PARSEC_MATRIX_FULL,
                            1, A->mb, params_tlr->maxrank*2, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_convert_d2s_UV_SP_ADT_IDX],
                            parsec_datatype_float_t, PARSEC_MATRIX_FULL,
                            1, A->mb, params_tlr->maxrank*2, A->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    parsec_add2arena(&taskpool->arenas_datatypes[PARSEC_convert_d2s_AR_ADT_IDX],
                            parsec_datatype_int_t, PARSEC_MATRIX_FULL,
                            1, 1, 1, 1, 
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    return convert_d2s_taskpool;
}

/**
 * @param [inout] the parsec object to destroy
*/
void hicma_parsec_convert_d2s_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_convert_d2s_taskpool_t *convert_d2s_taskpool = (parsec_convert_d2s_taskpool_t *)taskpool;
    parsec_del2arena(&convert_d2s_taskpool->arenas_datatypes[PARSEC_convert_d2s_FULL_DP_ADT_IDX]);
    parsec_del2arena(&convert_d2s_taskpool->arenas_datatypes[PARSEC_convert_d2s_FULL_SP_ADT_IDX]);
    parsec_del2arena(&convert_d2s_taskpool->arenas_datatypes[PARSEC_convert_d2s_UV_DP_ADT_IDX]);
    parsec_del2arena(&convert_d2s_taskpool->arenas_datatypes[PARSEC_convert_d2s_UV_SP_ADT_IDX]);
    parsec_del2arena(&convert_d2s_taskpool->arenas_datatypes[PARSEC_convert_d2s_AR_ADT_IDX]);
    parsec_taskpool_free(taskpool);
}

/**
 * @brief Init dcA
 * 
 * @param [inout] dcA: the data, already distributed and allocated
 * @param [in] dcAr: the data, already distributed and allocated
 */
int hicma_parsec_convert_d2s(parsec_context_t *parsec,
        hicma_parsec_data_t *data,
        hicma_parsec_params_t *params_tlr)
{
    parsec_taskpool_t *parsec_convert_d2s = NULL;
    parsec_convert_d2s = hicma_parsec_convert_d2s_New( parsec, data, params_tlr);
    if( parsec_convert_d2s != NULL ){
        parsec_context_add_taskpool(parsec, parsec_convert_d2s);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);
        hicma_parsec_convert_d2s_Destruct(parsec_convert_d2s);
    }

    return 0;
}

%}
