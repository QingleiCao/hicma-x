extern "C" %{
/**
 * @copyright (c) 2023-2025     Saint Louis University (SLU)
 * @copyright (c) 2023-2025     Massachusetts Institute of Technology (MIT)
 * @copyright (c) 2023-2025     Nvidia Corporation
 * @copyright (c) 2018-2025     King Abdullah University of Science and Technology (KAUST)
 * @copyright (c) 2018-2023     The University of Tennessee and The University of Tennessee Research Foundation
 *                              All rights reserved.
 **/

#include "hicma_parsec.h"

/**
 * @file potrf_L_dense_mp_band.jdf
 * @brief Mixed-Precision Cholesky Factorization with Band-Based Precision Switching
 * 
 * This file implements a hierarchical mixed-precision Cholesky factorization algorithm
 * that dynamically switches between double, single, and half precision based on
 * distance from the diagonal (band-based approach). The algorithm is designed for
 * distributed memory systems using the PaRSEC runtime.
 * 
 * Key Features:
 * - Mixed-precision computation (double/single/half precision)
 * - Band-based precision switching for optimal performance/accuracy trade-off
 * - GPU acceleration support with CUDA/HIP
 * - Hierarchical task scheduling for distributed systems
 * - Recursive factorization for small subproblems
 * 
 * Algorithm Overview:
 * The Cholesky factorization A = L*L^T is computed using a blocked algorithm where:
 * - Diagonal blocks use double precision for maximum accuracy
 * - Near-diagonal blocks use single precision for balanced performance/accuracy
 * - Far-off-diagonal blocks use half precision for maximum performance
 * - Precision switching is controlled by band_size_dense_dp and band_size_dense_sp parameters
 */

/* If using self-written convertor or from lapack */
#define OWN_CONVERTOR 0

/* Print more info for debugging */
static int print_more = 0;
static int print_more_gpu = 0;


/**
 * @brief Task Priority System for Hierarchical Scheduling
 * 
 * This section defines the priority system used for task scheduling in the mixed-precision
 * Cholesky factorization. Priorities are calculated based on the position of tasks in the
 * computation DAG to ensure optimal scheduling and load balancing.
 * 
 * Priority Formulas:
 * - potrf_hsdpotrf(k)    : (MT-k)^3     - Diagonal factorization tasks
 * - potrf_hsdsyrk(k,m)   : (MT-m)^3 + 3*(m-k) - Symmetric rank-k update tasks  
 * - potrf_hsdtrsm(m,k)   : (MT-m)^3 + 3*(m-k)*(2*MT-k-m-1) - Triangular solve tasks
 * - potrf_hsdgemm(m,n,k) : (MT-m)^3 + 3*(m-n)*(2*MT-m-n-1) + 6*(m-k) - Matrix multiply tasks
 * 
 * Maximum Priority Calculation:
 * (MT - PRI_CHANGE)^3 + 3*MT*(2*MT - PRI_CHANGE - 1) + 6*MT < (MT^3 + 6*MT^2 + 3*MT)
 * 
 * @warning If MT (matrix tile count) is greater than 1200, integer overflow may occur
 *          in priority calculations. Consider using 64-bit integers for large problems.
 */

%}

/**
 * @brief Global Variables and Parameters
 * 
 * This section defines the global variables and parameters used throughout the
 * mixed-precision Cholesky factorization algorithm.
 */
descA      [type = "parsec_tiled_matrix_t*" ]  /**< Matrix descriptor for the input/output matrix A */
params_tlr [type = "hicma_parsec_params_t *" ] /**< Algorithm parameters including precision bands and GPU settings */

PRI_CHANGE [type = "int" hidden = on default = 0 ] /**< Priority change threshold for task scheduling */
PRI_MAX    [type = "int" hidden = on default = "(descA->mt * ( 3 + descA->mt * ( 2 + descA->mt )))" ] /**< Maximum priority value */

/* Temporary buffers for data type conversion */
p_work_double        [ type = "parsec_memory_pool_t *" hidden = on default = NULL ] /**< Memory pool for double precision temporary buffers */
p_work_single        [ type = "parsec_memory_pool_t *" hidden = on default = NULL ] /**< Memory pool for single precision temporary buffers */

/* GPU workspace and device management */
ws_gpu               [ type = "void *" hidden = on default = NULL ] /**< GPU workspace for temporary computations */

nb_cuda_devices      [ type = "int"   hidden = on default = 0 ] /**< Number of available CUDA devices */
cuda_device_index    [ type = "int *" hidden = on default = "NULL"] /**< Array of CUDA device indices for load balancing */


/**************************************************
 * @brief Matrix Data Binding and GPU Load Balancing
 * 
 * This task binds matrix data to appropriate computational tasks based on the
 * mixed-precision band strategy. It also handles GPU load balancing by
 * distributing tasks across available CUDA devices.
 * 
 * The task determines the appropriate precision and computational path for each
 * matrix block based on its position relative to the diagonal:
 * - Diagonal blocks (m==n==0): Use double precision factorization
 * - Near-diagonal blocks: Use double precision for small bands, single for larger bands
 * - Off-diagonal blocks: Use appropriate precision based on distance from diagonal
 **************************************************/
potrf_bind_A(m, n)

// Execution space: Lower triangular matrix blocks only
m = 0 .. descA->nt-1  /**< Row index of matrix block */
n = 0 .. m            /**< Column index of matrix block (lower triangular) */

// Parallel partitioning: Each block is processed independently
:descA(m, n)

READ A <- descA(m, n)
       -> (m == 0 && n == 0) ? T potrf_hsdpotrf(0)                                                [ type_remote = DOUBLE ]
       -> (m < params_tlr->band_size_dense_dp && n == 0) ? C potrf_hsdtrsm(m, 0)                  [ type_remote = DOUBLE ]
       -> (m >= params_tlr->band_size_dense_dp && n == 0) ? C potrf_hsdtrsm(m, 0)                 [ type_remote = SINGLE ]
       -> (m == n && n > 0) ? T potrf_hsdsyrk(0, m)                                               [ type_remote = DOUBLE ]
       -> (m != n && n > 0 && m-n < params_tlr->band_size_dense_dp) ? C potrf_hsdgemm(m, n, 0)    [ type_remote = DOUBLE ]
       -> (m != n && n > 0 && m-n >= params_tlr->band_size_dense_dp) ? C potrf_hsdgemm(m, n, 0)   [ type_remote = SINGLE ]

BODY
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    /* GPU load balancing: Distribute tasks across available CUDA devices */
    if( nb_cuda_devices > 0 ) {
        int g = gpu_load_mixed_precision( m, n, descA->nt, params_tlr->P, nb_cuda_devices, 
                                         params_tlr->band_size_dense_dp, params_tlr->band_size_dense_sp);
        /* Advise the runtime to prefer the selected GPU device for this data */
        parsec_advise_data_on_device( _f_A->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
    }
#endif
}
END


/**************************************************
 * @brief Diagonal Block Cholesky Factorization
 * 
 * This task performs the Cholesky factorization of diagonal blocks A(k,k).
 * It is the core computational kernel that factorizes each diagonal block
 * using double precision for maximum accuracy.
 * 
 * The task supports both recursive and direct factorization:
 * - For large blocks: Uses recursive factorization with smaller subproblems
 * - For small blocks: Uses direct LAPACK/BLAS calls
 * - Supports GPU acceleration when available
 * 
 * @param k Block index along the diagonal (0 <= k < mt)
 **************************************************/
potrf_hsdpotrf(k) [high_priority = on] 

// Execution space: All diagonal blocks
k = 0 .. descA->mt-1  /**< Diagonal block index */

info = 0  /**< Error code for recursive factorization calls */

// Parallel partitioning: Each diagonal block is independent
:descA(k, k)

// Data flow: Read input, write output, trigger dependent tasks
RW T <- (k == 0) ? A potrf_bind_A(k, k) : T potrf_hsdsyrk(k-1, k)                                                      [ type_remote = DOUBLE ]
     -> (params_tlr->datatype_convert == 0) ? T potrf_hsdtrsm(k+1..descA->mt-1, k)                                       [ type_remote = DOUBLE ]
     -> (params_tlr->datatype_convert) ? T potrf_hsdtrsm(k+1..k+params_tlr->band_size_dense_dp-1, k)                     [ type_remote = DOUBLE ]
     -> (params_tlr->datatype_convert && k < descA->mt-params_tlr->band_size_dense_dp) ? T0 potrf_hsdpotrf_convert(k)    [ type_remote = DOUBLE ] 
     -> descA(k, k)                                                             

; (k >= (descA->mt - PRI_CHANGE)) ? (descA->mt - k) * (descA->mt - k) * (descA->mt - k) : PRI_MAX

BODY [type=CUDA]
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    /* GPU implementation: Delegate to GPU-specific Cholesky factorization */
    hicma_parsec_core_potrf_gpu( descA, params_tlr, ws_gpu, cuda_device, gpu_task, cuda_stream, T, k );
#endif
}
END

BODY [type=RECURSIVE]
{
    /* Recursive implementation for large diagonal blocks */
    int tempkm = k == descA->mt-1 ? descA->m - k*descA->mb : descA->mb;  /* Actual block size */
    int smallnb = params_tlr->HNB;  /* Recursive threshold */

    /* Use sequential CPU version for small blocks */
    if( tempkm <= smallnb ) {
        return PARSEC_HOOK_RETURN_NEXT;
    }

    if(DEBUG_INFO) printf("POTRF_Recursive: %d\n", k);

    /* Update operation counters for performance monitoring */
    unsigned long int cnt = hicma_parsec_op_counts('c', tempkm, 0, 0, 0);
    params_tlr->op_band[es->th_id] += cnt;
    params_tlr->op_path[es->th_id] += cnt;

    /* Print progress information */
    hicma_parsec_print_process( descA->mt, k, params_tlr->start_time_potrf );

    /* Create subtile descriptor for recursive factorization */
    subtile_desc_t *small_descT;
    parsec_taskpool_t *parsec_dpotrf;

    small_descT = subtile_desc_create( descA, k, k,
            smallnb, smallnb, 0, 0, tempkm, tempkm );
    small_descT->mat = T;

    /* Create recursive taskpool for smaller subproblem */
    parsec_dpotrf = dplasma_dpotrf_New(params_tlr->uplo, (parsec_tiled_matrix_t *)small_descT, &this_task->locals.info.value);

    /* Execute recursive call asynchronously */
    parsec_recursivecall((parsec_task_t*)this_task,
            parsec_dpotrf, dplasma_dpotrf_Destruct,
            1, small_descT);

    return PARSEC_HOOK_RETURN_ASYNC;
}
END

BODY
{
    /* CPU implementation: Direct Cholesky factorization */
    hicma_parsec_core_potrf_cpu( descA, params_tlr, es, T, k );
}
END

/**************************************************
 * @brief Data Type Conversion for Diagonal Blocks
 * 
 * This task converts diagonal block data from double precision to single precision
 * when the mixed-precision strategy requires it. This conversion is necessary
 * for blocks that are far enough from the diagonal to use single precision
 * in subsequent computations.
 * 
 * The conversion is performed using either a custom converter or LAPACK's
 * dlag2s function for optimal performance.
 * 
 * @param k Block index along the diagonal
 **************************************************/
potrf_hsdpotrf_convert(k) [high_priority = on]

// Execution space: Only blocks that need conversion (not in double precision band)
k = 0 .. descA->mt-1-params_tlr->band_size_dense_dp

// Parallel partitioning: Each block conversion is independent
:descA(k, k)

// Data flow: Read double precision input, write single precision output
READ T0 <- (params_tlr->datatype_convert == 1) ? T potrf_hsdpotrf(k) : NULL                                            [ type_remote = DOUBLE ]

RW T <- (params_tlr->datatype_convert == 1) ? NEW : NULL                                                               [ type = SINGLE ] 
     -> (params_tlr->datatype_convert == 1) ? T potrf_hsdtrsm(k+params_tlr->band_size_dense_dp..descA->mt-1, k)        [ type_remote = SINGLE ]

; (k >= (descA->mt - PRI_CHANGE)) ? (descA->mt - k) * (descA->mt - k) * (descA->mt - k) : PRI_MAX


BODY
{
    /* Convert datatype from double to single precision */
    if( 1 == params_tlr->datatype_convert ) {
        if( print_more ) fprintf(stderr, "potrf_convert CPU %d\n", k);
#if OWN_CONVERTOR
        /* Use custom binary converter for better performance */
        convert_d2s_binary_CPU(T, T0, descA->mb, descA->nb);
#else
        /* Use LAPACK converter for standard compatibility */
        LAPACKE_dlag2s( LAPACK_COL_MAJOR, descA->mb, descA->nb, T0, descA->mb, T, descA->mb );
#endif
    }
}
END


/**************************************************
 * @brief Triangular Solve (TRSM) for Mixed Precision
 * 
 * This task performs triangular solve operations A(m,k) = A(m,k) * A(k,k)^(-T)
 * using mixed precision based on the distance from the diagonal. The precision
 * is determined by the band-based strategy:
 * - Near-diagonal blocks: Use double precision
 * - Far-off-diagonal blocks: Use single precision
 * 
 * The task handles data type conversion when necessary and supports both
 * CPU and GPU implementations.
 * 
 * @param m Row index of the target block
 * @param k Column index of the source diagonal block
 **************************************************/
potrf_hsdtrsm(m, k) [high_priority = on] 

// Execution space: All off-diagonal blocks in lower triangular part
m = 1 .. descA->mt-1  /**< Row index of target block */
k = 0 .. m-1          /**< Column index of source diagonal block */

// Parallel partitioning: Each block solve is independent
: descA(m, k)

// Parameters
READ  T <- (params_tlr->datatype_convert == 0) ? T potrf_hsdpotrf(k)                                                      [ type_remote = DOUBLE ]
        <- (params_tlr->datatype_convert && m-k < params_tlr->band_size_dense_dp) ? T potrf_hsdpotrf(k)                   [ type_remote = DOUBLE ] 
        <- (params_tlr->datatype_convert && m-k >= params_tlr->band_size_dense_dp) ? T potrf_hsdpotrf_convert(k)          [ type_remote = SINGLE ] 

RW    C <- (k == 0 && (m-k) < params_tlr->band_size_dense_dp) ? A potrf_bind_A(m, k)                   [ type_remote = DOUBLE ]
        <- (k == 0 && (m-k) >= params_tlr->band_size_dense_dp) ? A potrf_bind_A(m, k)                  [ type_remote = SINGLE ]
        <- ((m-k) < params_tlr->band_size_dense_dp)? C potrf_hsdgemm(m, k, k-1)                        [ type_remote = DOUBLE ]
        <- C potrf_hsdgemm(m, k, k-1)                                                                  [ type_remote = SINGLE ]
        -> ((m-k) < params_tlr->band_size_dense_dp)? A potrf_hsdsyrk(k, m)                             [ type_remote = DOUBLE ]
        -> ((m-k) < params_tlr->band_size_dense_dp)? A potrf_hsdgemm(m, k+1..m-1, k)                   [ type_remote = DOUBLE ]
        -> ((m-k) < params_tlr->band_size_dense_dp && params_tlr->datatype_convert == 0)? B potrf_hsdgemm(m+1..descA->mt-1, m, k)                        [ type_remote = DOUBLE ]
        -> (m-k < params_tlr->band_size_dense_dp && params_tlr->datatype_convert) ? B potrf_hsdgemm(m+1 .. m+params_tlr->band_size_dense_dp-1, m, k)     [ type_remote = DOUBLE ]
        -> (m-k < params_tlr->band_size_dense_dp && params_tlr->datatype_convert) ? C0 potrf_hsdtrsm_convert(m, k)                                       [ type_remote = DOUBLE ]
        -> ((m-k) < params_tlr->band_size_dense_dp)? descA(m, k)                                       
        -> ((m-k) >= params_tlr->band_size_dense_dp)? A potrf_hsdsyrk(k, m)                            [ type_remote = SINGLE ]
        -> ((m-k) >= params_tlr->band_size_dense_dp)? A potrf_hsdgemm(m, k+1..m-1, k)                  [ type_remote = SINGLE ]
        -> ((m-k) >= params_tlr->band_size_dense_dp)? B potrf_hsdgemm(m+1..descA->mt-1, m, k)          [ type_remote = SINGLE ]
        -> ((m-k) >= params_tlr->band_size_dense_dp)? descA(m, k)                                     

//CTL ctl <- (params_tlr->lookahead > 0 && m > params_tlr->lookahead+k)? ctl potrf_hsdsyrk(k, k+1)

CTL ctl1 <- (params_tlr->lookahead > 2 && m > params_tlr->lookahead+k)? ctl1 potrf_hsdgemm(k+2, k+1, k)

CTL ctl_left  -> (params_tlr->left_looking > 0)? ctl_left potrf_hsdgemm(m, k+params_tlr->P*params_tlr->left_looking, 0)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * ((2 * descA->mt) - k - m - 1) * (m - k) : PRI_MAX

BODY [type=CUDA]
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    /* GPU implementation: Delegate to GPU-specific triangular solve */
    hicma_parsec_core_trsm_gpu( descA, params_tlr, ws_gpu, cuda_device, gpu_task, cuda_stream, T, C, m, k );
#endif
}
END

BODY
{
    /* CPU implementation: Mixed-precision triangular solve */
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;  /* Actual block height */
    int ldak = descA->mb;  /* Leading dimension of diagonal block A(k,k) */
    int ldam = descA->mb;  /* Leading dimension of target block A(m,k) */

    /* Choose precision based on distance from diagonal */
    if( m-k < params_tlr->band_size_dense_dp ) {
        /* Near-diagonal: Use double precision for maximum accuracy */
        CORE_dtrsm(PlasmaRight, PlasmaLower, PlasmaTrans, PlasmaNonUnit,
                   tempmm, descA->nb,
                   (double)1.0, T /*A(k, k)*/, ldak,
                                C /*A(m, k)*/, ldam);
    } else {
        /* Far-off-diagonal: Use single precision for better performance */
        if( 0 == params_tlr->datatype_convert ) {
            /* Convert diagonal block from double to single precision */
            float *T_s = (float *)parsec_private_memory_pop(p_work_single); 
#if OWN_CONVERTOR
            convert_d2s_binary_CPU(T_s, T, descA->mb, descA->nb);
#else
            LAPACKE_dlag2s( LAPACK_COL_MAJOR, descA->mb, descA->nb, T, descA->mb, T_s, descA->mb );
#endif

            /* Perform triangular solve in single precision */
            CORE_strsm(PlasmaRight, PlasmaLower, PlasmaTrans, PlasmaNonUnit,
                       tempmm, descA->nb,
                       (float)1.0, T_s /*A(k, k)*/, ldak,
                                   C   /*A(m, k)*/, ldam);

            /* Return temporary buffer to memory pool */
            parsec_private_memory_push(p_work_single, T_s);
        } else {
            /* Diagonal block already in single precision */
            CORE_strsm(PlasmaRight, PlasmaLower, PlasmaTrans, PlasmaNonUnit,
                       tempmm, descA->nb,
                       (float)1.0, T /*A(k, k)*/, ldak,
                                   C /*A(m, k)*/, ldam);
       }
    }

}
END

/**************************************************
 *            potrf_hsdtrsm_convert                 *
 **************************************************/
potrf_hsdtrsm_convert(m, k) [high_priority = on]

// Execution space
m = 1 .. descA->mt-1
k = %{ return parsec_imax(m-params_tlr->band_size_dense_dp+1, 0); %} .. m-1

// Parallel partitioning
: descA(m, k)

READ C0 <- (params_tlr->datatype_convert == 1) ? C potrf_hsdtrsm(m, k) : NULL                                                                                              [ type_remote = DOUBLE ]

RW   C  <- (params_tlr->datatype_convert == 1 && m+params_tlr->band_size_dense_dp < descA->lmt) ? NEW : NULL                                                               [ type = SINGLE ]
        -> (params_tlr->datatype_convert == 1 && m+params_tlr->band_size_dense_dp < descA->lmt) ? B potrf_hsdgemm(m+params_tlr->band_size_dense_dp..descA->mt-1, m, k)     [ type_remote = SINGLE ]

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * ((2 * descA->mt) - k - m - 1) * (m - k) : PRI_MAX

BODY
{
    if( m+params_tlr->band_size_dense_dp < descA->lmt && 1 == params_tlr->datatype_convert ) {
        int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
        if( print_more ) printf("trsm_convert CPU %d %d\n", m, k);
        /* Convert datatype */
#if OWN_CONVERTOR
        convert_d2s_binary_CPU(C, C0, descA->mb, descA->nb);
#else
        LAPACKE_dlag2s( LAPACK_COL_MAJOR, descA->mb, descA->nb, C0, descA->mb, C, descA->mb );
#endif
    }
}
END


/**************************************************
 * @brief Symmetric Rank-K Update (SYRK) for Mixed Precision
 * 
 * This task performs symmetric rank-k updates A(m,m) = A(m,m) - A(m,k)*A(m,k)^T
 * using mixed precision based on the distance from the diagonal. The precision
 * is determined by the band-based strategy:
 * - Near-diagonal blocks: Use double precision
 * - Far-off-diagonal blocks: Use single precision with conversion
 * 
 * The task updates diagonal blocks by subtracting the contribution from
 * off-diagonal blocks, maintaining the symmetric structure of the matrix.
 * 
 * @param k Column index of the source block
 * @param m Row/column index of the target diagonal block
 **************************************************/
potrf_hsdsyrk(k, m) [high_priority = on]

// Execution space: All diagonal blocks except the first
k = 0   .. descA->mt-2  /**< Column index of source block */
m = k+1 .. descA->mt-1  /**< Row/column index of target diagonal block */

// Parallel partitioning: Each diagonal block update is independent
: descA(m, m)

//Parameters
READ  A <- (m-k < params_tlr->band_size_dense_dp)? C potrf_hsdtrsm(m, k)         [ type_remote = DOUBLE ]
        <- C potrf_hsdtrsm(m, k)                                                 [ type_remote = SINGLE ]

RW    T <- (k == 0)   ? A potrf_bind_A(m, m) : T potrf_hsdsyrk(k-1, m)           [ type_remote = DOUBLE ]
        -> (m == k+1) ? T potrf_hsdpotrf(m)  : T potrf_hsdsyrk(k+1, m)           [ type_remote = DOUBLE ]

//CTL ctl -> (params_tlr->lookahead > 0 && m == k+1)? ctl potrf_hsdtrsm(params_tlr->lookahead+m .. descA->mt-1, k)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * (m - k) : PRI_MAX

BODY [type=CUDA]
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    /* GPU implementation: Delegate to GPU-specific symmetric rank-k update */
    int Arank = 0;  /* Rank parameter (not used in dense case) */
    hicma_parsec_core_syrk_gpu( descA, params_tlr, ws_gpu, cuda_device, gpu_task, cuda_stream, T, A, m, k, Arank );
#endif
}
END

BODY
{
    /* CPU implementation: Mixed-precision symmetric rank-k update */
    if( print_more ) fprintf(stderr, "SYRK %d %d before: params_tlr->lookahead %d, sendless %d, params_tlr->band_size_dense_dp %d\n", m, k, params_tlr->lookahead, params_tlr->datatype_convert, params_tlr->band_size_dense_dp);

    int tempmm = m == descA->mt-1 ? descA->m - m*descA->mb : descA->mb;  /* Actual block size */
    int ldam = descA->mb;  /* Leading dimension of source block A(m,k) */

    /* Choose precision based on distance from diagonal */
    if( (m-k) >= params_tlr->band_size_dense_dp ) {
        /* Far-off-diagonal: Convert single to double precision for accuracy */
        double *A_d = (double *)parsec_private_memory_pop(p_work_double); 
#if OWN_CONVERTOR
        convert_s2d_binary_CPU(A_d, A, descA->mb, descA->nb);
#else
        LAPACKE_slag2d( LAPACK_COL_MAJOR, descA->mb, descA->nb, A, descA->mb, A_d, descA->mb );
#endif

        /* Perform symmetric rank-k update in double precision */
        CORE_dsyrk(PlasmaLower, PlasmaNoTrans,
                   tempmm, descA->mb,
                   (double)-1.0, A_d /*A(m, k)*/, ldam,
                   (double) 1.0, T /*A(m, m)*/, ldam);

        /* Return temporary buffer to memory pool */
        parsec_private_memory_push(p_work_double, A_d);
    } else {
        /* Near-diagonal: Use double precision directly */
        CORE_dsyrk(PlasmaLower, PlasmaNoTrans,
                   tempmm, descA->mb,
                   (double)-1.0, A /*A(m, k)*/, ldam,
                   (double) 1.0, T /*A(m, m)*/, ldam);
    }

}
END

/**************************************************
 * @brief Matrix Multiplication (GEMM) for Mixed Precision
 * 
 * This task performs matrix multiplications A(m,n) = A(m,n) - A(m,k)*A(n,k)^T
 * using mixed precision based on the distance from the diagonal. The precision
 * strategy uses three levels:
 * - Near-diagonal blocks: Use double precision
 * - Medium-distance blocks: Use single precision  
 * - Far-off-diagonal blocks: Use half precision for maximum performance
 * 
 * The task supports complex data type conversions and GPU acceleration with
 * tensor cores for half-precision operations.
 * 
 * @param k Column index of the source blocks
 * @param m Row index of the target block
 * @param n Column index of the target block
 **************************************************/
potrf_hsdgemm(m, n, k)

// Execution space: All off-diagonal blocks in lower triangular part
k = 0   .. descA->mt-3  /**< Column index of source blocks */
m = k+2 .. descA->mt-1  /**< Row index of target block */
n = k+1 .. m-1          /**< Column index of target block */

// Parallel partitioning: Each block multiplication is independent
: descA(m, n)

// Parameters
READ  A <- (m-k < params_tlr->band_size_dense_dp)? C potrf_hsdtrsm(m, k)                         [ type_remote = DOUBLE ]
        <- C potrf_hsdtrsm(m, k)                                                                 [ type_remote = SINGLE ]

READ   B <- (n-k < params_tlr->band_size_dense_dp && m-n < params_tlr->band_size_dense_dp) ? C potrf_hsdtrsm(n, k)                                              [ type_remote = DOUBLE ]
         <- (n-k < params_tlr->band_size_dense_dp && m-n >= params_tlr->band_size_dense_dp && params_tlr->datatype_convert == 0) ? C potrf_hsdtrsm(n, k)          [ type_remote = DOUBLE ]
         <- (n-k < params_tlr->band_size_dense_dp && m-n >= params_tlr->band_size_dense_dp && params_tlr->datatype_convert) ? C potrf_hsdtrsm_convert(n, k)       [ type_remote = SINGLE ]
         <- C potrf_hsdtrsm(n, k)                                                                [ type_remote = SINGLE ]

RW    C <- (k == 0 && m-n < params_tlr->band_size_dense_dp)? A potrf_bind_A(m, n)                [ type_remote = DOUBLE ]
        <- (k == 0 && m-n >= params_tlr->band_size_dense_dp)? A potrf_bind_A(m, n)               [ type_remote = SINGLE ]
        <- (m-n < params_tlr->band_size_dense_dp)? C potrf_hsdgemm(m, n, k-1)                    [ type_remote = DOUBLE ] 
        <- (m-n < params_tlr->band_size_dense_sp)? C potrf_hsdgemm(m, n, k-1)                    [ type_remote = SINGLE ] 
        <- C potrf_hsdgemm(m, n, k-1)                                              [ type = HALF ] 
        -> (n == k+1 && m-n < params_tlr->band_size_dense_dp)? C potrf_hsdtrsm(m, n)             [ type_remote = DOUBLE ]
        -> (n == k+1 && m-n >= params_tlr->band_size_dense_dp)? C potrf_hsdtrsm(m, n)            [ type_remote = SINGLE ]
        -> (n != k+1 && m-n < params_tlr->band_size_dense_dp)? C potrf_hsdgemm(m, n, k+1)        [ type_remote = DOUBLE ]
        -> (n != k+1 && m-n >= params_tlr->band_size_dense_dp && m-n < params_tlr->band_size_dense_sp)? C potrf_hsdgemm(m, n, k+1)             [ type_remote = SINGLE ]
        -> (n != k+1 && m-n >= params_tlr->band_size_dense_sp)? C potrf_hsdgemm(m, n, k+1)                                                     [ type_remote = HALF ]

CTL ctl1 -> (params_tlr->lookahead > 2 && m == k+2 && n == k+1)? ctl1 potrf_hsdtrsm(k+params_tlr->lookahead+1 .. descA->mt-1, k)

CTL ctl_left <- (params_tlr->left_looking > 0 && k == 0 && n >= params_tlr->P*params_tlr->left_looking)? ctl_left potrf_hsdtrsm(m, n-params_tlr->P*params_tlr->left_looking)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * ((2 * descA->mt) - m - n - 3) * (m - n) + 6 * (m - k) : PRI_MAX

BODY [type=CUDA]
{
#if defined(PARSEC_HAVE_DEV_CUDA_SUPPORT)
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldam = descA->mb; 
    int ldan = descA->mb; 
    const double alpha_double = (double)-1.0;
    const double beta_double = (double)1.0;
    const float alpha_float = (float)-1.0;
    const float beta_float = (float)1.0;
    if( print_more_gpu ) printf("GPU_gemm: %d %d %d; params_tlr->band_size_dense_dp: %d\n", m, n, k, params_tlr->band_size_dense_dp);

    void *A_use = A;
    void *B_use = B;
    void *A_d, *A_s, *A_h, *B_d, *B_s, *B_h, *C_s, *C_h, *buffer_mbr, *buffer_rr;
    cublasStatus_t status;

    /* Find workspace */
    parsec_potrf_workspace_t *_ws_gpu = (parsec_potrf_workspace_t *)ws_gpu;
    parsec_potrf_stream_workspace_t *stream_found = lookup_gpu_workspace(cuda_device, cuda_stream, _ws_gpu);

    /* Get handle_cublas */
    cublasHandle_t handle = stream_found->handle_cublas;
    if( DENSE_HP == params_tlr->decisions[n*descA->lmt+m] ) {
        handle = stream_found->handle_cublas_tensor;
    }
    cublasSetStream( handle, parsec_body.stream );

    /* Get the temporary buffer on GPU */
    A_d = (double *)stream_found->gpu_buffer_A;
    A_s = (float *)stream_found->gpu_buffer_A;
    A_h = (float *)stream_found->gpu_buffer_A;

    B_d = (double *)stream_found->gpu_buffer_B;
    B_s = (float *)stream_found->gpu_buffer_B;
    B_h = (float *)stream_found->gpu_buffer_B;

    C_s = (float *)stream_found->gpu_buffer_C;
    C_h = (float *)stream_found->gpu_buffer_C;

    if( m-n < params_tlr->band_size_dense_dp ) {
        if( m-k >= params_tlr->band_size_dense_dp ) {
            /* Convert datatype */
            float2double_GPU( descA->mb, descA->nb, A, descA->mb, A_d, descA->mb, parsec_body.stream );
        }

        if( n-k >= params_tlr->band_size_dense_dp ) {
            /* Convert datatype */
            float2double_GPU( descA->mb, descA->nb, B, descA->mb, B_d, descA->mb, parsec_body.stream );
        }

        status = cublasDgemm( handle, CUBLAS_OP_N, CUBLAS_OP_T,
                 tempmm, descA->mb, descA->mb, 
                 &alpha_double, ((m-k >= params_tlr->band_size_dense_dp)? (double *)A_d: (double *)A), ldam,
                                ((n-k >= params_tlr->band_size_dense_dp)? (double *)B_d: (double *)B), ldan,
                 &beta_double,  (double *)C, ldam );

    } else if( m-n < params_tlr->band_size_dense_sp ) {
        if( n-k < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert ) {
            /* Convert datatype */
            double2float_GPU( descA->mb, descA->nb, B, descA->mb, B_s, descA->mb, parsec_body.stream );
        }

        status = cublasSgemm( handle, CUBLAS_OP_N, CUBLAS_OP_T,
                 tempmm, descA->mb, descA->mb,
                 &alpha_float, (float *)A, ldam,
                               (((n-k) < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert )? (float *)B_s: (float *)B), ldan,
                 &beta_float,  (float *)C, ldam );

    } else {
        /* Convert A from double to half */
        float2half_GPU( descA->mb, descA->nb, A, descA->mb, A_h, descA->mb, parsec_body.stream );

        /* Convert B from double to half */
        if( n-k < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert ) {
            /* Convert datatype */
            double2half_GPU( descA->mb, descA->nb, B, descA->mb, B_h, descA->mb, parsec_body.stream );

        } else {
            /* Convert datatype */
            float2half_GPU( descA->mb, descA->nb, B, descA->mb, B_h, descA->mb, parsec_body.stream );
        }

        /* First local GEMM convert C from single to half */
        if( 0 == k && (params_tlr->tensor_gemm & MASK_TF16_A16_B16_C16_OP32
                    || params_tlr->tensor_gemm & MASK_TF16_A16_B16_C16_OP16) ) {
            /* Convert datatype */
            float2half_GPU( descA->mb, descA->nb, C, descA->mb, C_h, descA->mb, parsec_body.stream );

            /* Copy C_h to C */
            memcpy_half_GPU( descA->mb, descA->nb, C_h, C, parsec_body.stream );
        }

        /* HGEMM */
        status = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                (int64_t)tempmm, (int64_t)descA->mb, (int64_t)descA->mb,
                &alpha_float, A_h, CUDA_R_16F, (int64_t)ldam,
                              B_h, CUDA_R_16F, (int64_t)ldan,
                &beta_float,  C,   (params_tlr->tensor_gemm & MASK_TF16_A16_B16_C32_OP32)? CUDA_R_32F: CUDA_R_16F, (int64_t)ldam,
                (params_tlr->tensor_gemm & MASK_TF16_A16_B16_C16_OP16)? CUDA_R_16F: CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);

        /* After last local GEMM convert C from half to single */
        if( n-1 == k && (params_tlr->tensor_gemm & MASK_TF16_A16_B16_C16_OP32
                    || params_tlr->tensor_gemm & MASK_TF16_A16_B16_C16_OP16) ) {
            /* Convert datatype */
            half2float_GPU( descA->mb, descA->nb, C, descA->mb, C_s, descA->mb, parsec_body.stream );

            /* Copy C_s to C */
            memcpy_float_GPU( descA->mb, descA->nb, C_s, C, parsec_body.stream );
        }
    }
#endif

}
END

BODY
{
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldam = descA->mb; 
    int ldan = descA->mb; 

    if( m-n < params_tlr->band_size_dense_dp ) {
        double *A_d, *B_d;
        if( m-k >= params_tlr->band_size_dense_dp ) {
            A_d = (double *)parsec_private_memory_pop(p_work_double); 
#if OWN_CONVERTOR
            convert_s2d_binary_CPU(A_d, A, descA->mb, descA->nb);
#else
            LAPACKE_slag2d( LAPACK_COL_MAJOR, descA->mb, descA->nb, A, descA->mb, A_d, descA->mb );
#endif
        }

        if( n-k >= params_tlr->band_size_dense_dp ) {
            B_d = (double *)parsec_private_memory_pop(p_work_double);
#if OWN_CONVERTOR
            convert_s2d_binary_CPU(B_d, B, descA->mb, descA->nb);
#else
            LAPACKE_slag2d( LAPACK_COL_MAJOR, descA->mb, descA->nb, B, descA->mb, B_d, descA->mb );
#endif
        }

        CORE_dgemm(PlasmaNoTrans, PlasmaTrans,
                   tempmm, descA->mb, descA->mb,
                   (double)-1.0, ((m-k >= params_tlr->band_size_dense_dp)? A_d: A) /*A(m, k)*/, ldam,
                                 ((n-k >= params_tlr->band_size_dense_dp)? B_d: B) /*A(n, k)*/, ldan,
                   (double) 1.0, C /*A(m, n)*/, ldam);

        if( m-k >= params_tlr->band_size_dense_dp ) parsec_private_memory_push(p_work_double, A_d);
        if( n-k >= params_tlr->band_size_dense_dp ) parsec_private_memory_push(p_work_double, B_d);
    } else {
        float *B_s;
        if( n-k < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert ) {
            B_s = (float *)parsec_private_memory_pop(p_work_single); 
#if OWN_CONVERTOR
            convert_d2s_binary_CPU(B_s, B, descA->mb, descA->nb);
#else
            LAPACKE_dlag2s( LAPACK_COL_MAJOR, descA->mb, descA->nb, B, descA->mb, B_s, descA->mb );
#endif
        }

        CORE_sgemm(PlasmaNoTrans, PlasmaTrans,
                   tempmm, descA->mb, descA->mb,
                   (float)-1.0,  A                          /*A(m, k)*/, ldam,
                                ((n-k < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert)? B_s: B) /*A(n, k)*/, ldan,
                   (float) 1.0, C /*A(m, n)*/, ldam);

        if( n-k < params_tlr->band_size_dense_dp && 0 == params_tlr->datatype_convert ) parsec_private_memory_push(p_work_single, B_s);
    }

    printlog("CORE_dgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaTrans ),
             tempmm, descA->mb, descA->mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);
}
END
